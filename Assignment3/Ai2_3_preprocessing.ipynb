{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ai2_3_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC01sicFvLMx",
        "outputId": "6cc20904-d7d5-4302-c01f-ae5e996d3d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import os\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from PIL import Image\n",
        "import urllib\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import time\n",
        "import glob\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "import folium\n",
        "from itertools import cycle, islice\n",
        "from pandas import options\n",
        "import warnings\n",
        "import pickle\n",
        "import re\n",
        "import nltk\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from matplotlib.pyplot import figure\n",
        "from nltk.corpus import stopwords \n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "wyZuw5ow5cJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load dataframes"
      ],
      "metadata": {
        "id": "drKqk52L5fpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "B_ZcYVVBwT-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/My Drive/\"\n",
        "\n",
        "#Read data from csv file \n",
        "df = pd.read_csv(path + 'vaccine_train_set.csv',nrows=15976) \n",
        "df = df.dropna()\n",
        "df"
      ],
      "metadata": {
        "id": "Q4jFGwprwC5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/My Drive/\"\n",
        "\n",
        "#Read data from csv file \n",
        "df2 = pd.read_csv(path + 'vaccine_validation_set.csv',nrows=2282) \n",
        "df2 = df2.dropna()\n",
        "df2"
      ],
      "metadata": {
        "id": "zBOHTD-ayZWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.drop(['Unnamed: 0'], axis=1)\n",
        "df2"
      ],
      "metadata": {
        "id": "endg7aAJyg3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['Unnamed: 0'], axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "JevjyF3mwtC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Remove unneccesary characters for train and validation set"
      ],
      "metadata": {
        "id": "ioAq5uw55k89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweet'] = df['tweet'].str.lower()\n",
        "#Define function for removing special characters\n",
        "def remove_special_characters(tweet, remove_digits=True):\n",
        "    tweet=re.sub(r'[^a-zA-z0-9\\s]','',tweet)\n",
        "    return tweet\n",
        "df['tweet'] = df['tweet'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
        "#Apply function on review column\n",
        "df['tweet']=df['tweet'].apply(remove_special_characters)\n",
        "\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "vz-4zzUXxKyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['tweet'] = df2['tweet'].str.lower()\n",
        "#Define function for removing special characters\n",
        "def remove_special_characters(tweet, remove_digits=True):\n",
        "    tweet=re.sub(r'[^a-zA-z0-9\\s]','',tweet)\n",
        "    return tweet\n",
        "df2['tweet'] = df2['tweet'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
        "#Apply function on review column\n",
        "df2['tweet']=df2['tweet'].apply(remove_special_characters)\n",
        "\n",
        "\n",
        "df2"
      ],
      "metadata": {
        "id": "JfRdTWZ3yw3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].value_counts()"
      ],
      "metadata": {
        "id": "qFZDMCIVxXnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['label'].value_counts()"
      ],
      "metadata": {
        "id": "yWJ8kds3y11m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Remove links"
      ],
      "metadata": {
        "id": "26UFg5L65-t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#Remove tags and links \n",
        "tag = re.compile(r'<[^>]+>')\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(lambda x: tag.sub('', x)) #removing html labels\n",
        "\n",
        "df['tweet'] = df['tweet'].replace(r'http\\S+', '', regex=True).replace(r'www.\\S+', '', regex=True).replace(r'http\\S+', '', regex=True).replace(r'\"', '', regex=True)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "8xy2-qdJxjUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['tweet'] = df2['tweet'].apply(lambda x: tag.sub('', x)) #removing html labels\n",
        "\n",
        "df2['tweet'] = df2['tweet'].replace(r'http\\S+', '', regex=True).replace(r'www.\\S+', '', regex=True).replace(r'http\\S+', '', regex=True).replace(r'\"', '', regex=True)\n"
      ],
      "metadata": {
        "id": "1Rn8j2Ray55U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lemmatizer"
      ],
      "metadata": {
        "id": "MFe7Un7e53u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(w, pos=\"v\") for w in w_tokenizer.tokenize(text)])\n",
        "\n",
        "df['tweet'] = df.tweet.apply(lemmatize_text).copy()\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "3WIdzkGBx1P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(w, pos=\"v\") for w in w_tokenizer.tokenize(text)])\n",
        "\n",
        "df2['tweet'] = df2.tweet.apply(lemmatize_text).copy()\n",
        "\n",
        "df2"
      ],
      "metadata": {
        "id": "RTcNexvizBcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create test set \n",
        "####Splitted from train set"
      ],
      "metadata": {
        "id": "zwxPE-Ga6KvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = pd.DataFrame(df, columns = ['tweet']) \n",
        "y = pd.DataFrame(df, columns = ['label']) \n",
        "\n",
        "# Split dataset to train and test set.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.15, random_state=42)\n",
        "\n",
        "X_val = pd.DataFrame(df2, columns = ['tweet']) \n",
        "Y_val = pd.DataFrame(df2, columns = ['label']) \n",
        "\n",
        "print(\"Shape of x_train: \", X_train.shape)\n",
        "print(\"Shape of y_train: \", Y_train.shape)\n",
        "print(\"Shape of x_test:  \", X_test.shape)\n",
        "print(\"Shape of y_test:  \", Y_test.shape)\n",
        "print(\"Shape of x_val:  \", X_val.shape)\n",
        "print(\"Shape of y_val:  \", Y_val.shape)"
      ],
      "metadata": {
        "id": "0qLqUqQAzGhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the dataframes to files"
      ],
      "metadata": {
        "id": "lNqJnfW-6ZjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_file = open('/content/drive/My Drive/X_train.pkl', 'wb')\n",
        "pickle.dump(X_train,x_train_file)\n",
        "x_train_file.close()\n",
        "\n",
        "x_test_file = open('/content/drive/My Drive/X_test.pkl', 'wb')\n",
        "pickle.dump(X_test,x_test_file)\n",
        "x_test_file.close()\n",
        "\n",
        "y_train_file = open('/content/drive/My Drive/Y_train.pkl', 'wb')\n",
        "pickle.dump(Y_train,y_train_file)\n",
        "y_train_file.close()\n",
        "\n",
        "y_test_file = open('/content/drive/My Drive/Y_test.pkl', 'wb')\n",
        "pickle.dump(Y_test,y_test_file)\n",
        "y_test_file.close()\n",
        "\n",
        "x_val_file = open('/content/drive/My Drive/X_val.pkl', 'wb')\n",
        "pickle.dump(X_val,x_val_file)\n",
        "x_val_file.close()\n",
        "\n",
        "y_val_file = open('/content/drive/My Drive/Y_val.pkl', 'wb')\n",
        "pickle.dump(Y_val,y_val_file)\n",
        "y_val_file.close()"
      ],
      "metadata": {
        "id": "Z0MHZcde0LjJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}